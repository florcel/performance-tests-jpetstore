name: Performance Tests

on:
  workflow_dispatch:
    inputs:
      users:
        description: 'N√∫mero de usuarios concurrentes'
        required: false
        default: '10'
        type: string
      duration:
        description: 'Duraci√≥n de la prueba en segundos'
        required: false
        default: '300'
        type: string
      target_url:
        description: 'URL objetivo para las pruebas'
        required: false
        default: 'https://petstore.octoperf.com'
        type: string

env:
  JMETER_VERSION: '5.6.3'
  USERS: ${{ github.event.inputs.users || '10' }}
  DURATION: ${{ github.event.inputs.duration || '300' }}
  TARGET_URL: ${{ github.event.inputs.target_url || 'https://petstore.octoperf.com' }}

jobs:
  performance-tests:
    runs-on: [self-hosted, Windows, X64]

    strategy:
      matrix:
        test-scenario: [light, medium, heavy]
        include:
          - test-scenario: light
            users: 5
            duration: 120
          - test-scenario: medium
            users: 20
            duration: 300
          - test-scenario: heavy
            users: 50
            duration: 600

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Cache JMeter
        id: cache
        uses: actions/cache@v4
        with:
          path: apache-jmeter-${{ env.JMETER_VERSION }}
          key: jmeter-${{ env.JMETER_VERSION }}

      - name: Download JMeter (Windows)
        if: steps.cache.outputs.cache-hit != 'true'
        shell: pwsh
        run: |
          $ver = "${{ env.JMETER_VERSION }}"
          $tgz = "apache-jmeter-$ver.tgz"
          Invoke-WebRequest -Uri "https://archive.apache.org/dist/jmeter/binaries/$tgz" -OutFile $tgz
          tar -xzf $tgz
          Remove-Item $tgz

      - name: Create results directory
        shell: pwsh
        run: New-Item -ItemType Directory -Force -Path results | Out-Null

      - name: Run performance tests (${{ matrix.test-scenario }})
        shell: pwsh
        run: |
          $JMETER = ".\apache-jmeter-${{ env.JMETER_VERSION }}\bin\jmeter.bat"
          & $JMETER -n `
            -t "jpetstore_jmeter_testplan.jmx" `
            -l "results\results_${{ matrix.test-scenario }}.jtl" `
            -Jusers=${{ matrix.users }} `
            -Jduration=${{ matrix.duration }} `
            -Jtarget_url="${{ env.TARGET_URL }}" `
            -Jjmeter.save.saveservice.output_format=csv

      - name: Generate HTML report
        shell: pwsh
        run: |
          $JMETER = ".\apache-jmeter-${{ env.JMETER_VERSION }}\bin\jmeter.bat"
          & $JMETER -g "results\results_${{ matrix.test-scenario }}.jtl" -o "results\report_${{ matrix.test-scenario }}"

      - name: Extract key metrics
        id: metrics
        shell: pwsh
        run: |
          $file = "results\results_${{ matrix.test-scenario }}.jtl"
          if (Test-Path $file) {
            $lines = Get-Content $file
            if ($lines.Count -gt 1) {
              $sum = 0.0; $count = 0; $errors = 0
              foreach ($line in $lines | Select-Object -Skip 1) {
                $cols = $line.Split(',')
                if ($cols.Count -ge 8) {
                  $sum += [double]$cols[1]
                  $count++
                  if ($cols[7].Trim().ToLower() -eq 'false') { $errors++ }
                }
              }
              if ($count -gt 0) {
                $avg = $sum / $count
                $errorRate = 100.0 * $errors / $count
                $throughput = $count / [double]${{ matrix.duration }}
              } else { $avg=0; $errorRate=0; $throughput=0 }
            } else { $avg=0; $errorRate=100; $throughput=0 }
          } else { $avg=0; $errorRate=100; $throughput=0 }
          "avg_response_time=$avg" >> $env:GITHUB_OUTPUT
          "error_rate=$errorRate" >> $env:GITHUB_OUTPUT
          "throughput=$throughput" >> $env:GITHUB_OUTPUT

      - name: Performance thresholds check
        shell: pwsh
        run: |
          $avg = [double]"${{ steps.metrics.outputs.avg_response_time }}"
          $err = [double]"${{ steps.metrics.outputs.error_rate }}"
          switch ("${{ matrix.test-scenario }}") {
            'light'  { $maxRt=1000; $maxErr=1 }
            'medium' { $maxRt=2000; $maxErr=2 }
            'heavy'  { $maxRt=5000; $maxErr=5 }
          }
          Write-Host "üîç Verificando umbrales para ${{ matrix.test-scenario }}"
          if ($avg -gt $maxRt) { Write-Error "‚ùå FALLO: Avg ${avg}ms > ${maxRt}ms"; exit 1 }
          if ($err -gt $maxErr) { Write-Error "‚ùå FALLO: Error ${err}% > ${maxErr}%"; exit 1 }
          Write-Host "‚úÖ Umbrales OK"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ matrix.test-scenario }}
          path: |
            results\results_${{ matrix.test-scenario }}.jtl
            results\report_${{ matrix.test-scenario }}\
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            let comment = `## üìä Resultados de Performance Tests - ${{ matrix.test-scenario }}\n\n`;
            comment += `**Configuraci√≥n:**\n`;
            comment += `- Usuarios concurrentes: ${{ matrix.users }}\n`;
            comment += `- Duraci√≥n: ${{ matrix.duration }}s\n`;
            comment += `- URL objetivo: ${{ env.TARGET_URL }}\n\n`;
            comment += `**M√©tricas:**\n`;
            comment += `- ‚è±Ô∏è Tiempo de respuesta promedio: ${{ steps.metrics.outputs.avg_response_time }}ms\n`;
            comment += `- ‚ùå Tasa de errores: ${{ steps.metrics.outputs.error_rate }}%\n`;
            comment += `- üìà Throughput: ${{ steps.metrics.outputs.throughput }} TPS\n\n`;
            const avg = parseFloat('${{ steps.metrics.outputs.avg_response_time }}');
            const err = parseFloat('${{ steps.metrics.outputs.error_rate }}');
            let status = '‚úÖ';
            if (avg > 2000 || err > 2) status = '‚ö†Ô∏è';
            if (avg > 5000 || err > 5) status = '‚ùå';
            comment += `**Estado:** ${status}\n\n`;
            comment += `üìÅ [Ver reporte completo](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  performance-summary:
    runs-on: [self-hosted, Windows, X64]
    needs: performance-tests
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Generate summary report
        shell: pwsh
        run: |
          Set-Content -Path performance-summary.md -Value "# üìä Performance Tests Summary"
          Add-Content -Path performance-summary.md -Value ""
          Add-Content -Path performance-summary.md -Value "## Test Results Overview"
          Add-Content -Path performance-summary.md -Value ""
          foreach ($scenario in 'light','medium','heavy') {
            $base = Join-Path artifacts ("performance-results-$scenario")
            if (Test-Path $base) {
              Add-Content performance-summary.md "### $scenario Scenario"
              Add-Content performance-summary.md ""
              $jtl = Join-Path $base ("results_${scenario}.jtl")
              if (Test-Path $jtl) { Add-Content performance-summary.md "‚úÖ Test completed successfully" }
              else { Add-Content performance-summary.md "‚ùå Test failed or no results found" }
              Add-Content performance-summary.md ""
            }
          }

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: performance-summary.md
          retention-days: 30
